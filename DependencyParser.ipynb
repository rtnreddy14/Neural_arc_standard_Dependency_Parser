{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse_incr\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DependencyParsingDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.data = self.load_data(file_path)\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        data = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for sentence in parse_incr(file):\n",
    "                #print(sentence)\n",
    "                transitions = self.generate_transitions(sentence)\n",
    "                data.append(transitions)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def oracle(self, stack, buffer, sentence):\n",
    "        if len(stack)<2 :\n",
    "            return 'SHIFT'  # This ensures we don't try to access buffer[0] when buffer is empty\n",
    "        #print(stack,buffer)\n",
    "        top_of_stack = stack[-1] if stack else None\n",
    "        first_in_buffer = buffer[0] if buffer else None\n",
    "\n",
    "        if top_of_stack is not None and first_in_buffer is not None:\n",
    "            buffer_head_idx = sentence[first_in_buffer - 1]['head']  # Adjusting index for zero-based list access\n",
    "            stack_head_idx = sentence[top_of_stack - 1]['head']      # Adjusting index for zero-based list access\n",
    "\n",
    "            if buffer_head_idx == top_of_stack:\n",
    "                return 'RIGHT-ARC'\n",
    "            elif stack_head_idx == first_in_buffer:\n",
    "                return 'LEFT-ARC'\n",
    "\n",
    "        return 'SHIFT'\n",
    "    \n",
    "\n",
    "    def generate_transitions(self, sentence):\n",
    "        transitions = []\n",
    "        stack = [0]  # Start with ROOT at the stack\n",
    "\n",
    "\n",
    "            # Initialize buffer to handle multi-word tokens and null tokens\n",
    "        buffer = deque()\n",
    "        for token in sentence:\n",
    "            if isinstance(token['id'], tuple) and token['form'] == '-':\n",
    "                continue  # Ignore null tokens if represented by '-'\n",
    "            elif isinstance(token['id'], tuple):\n",
    "                buffer.append(token['id'][0])  # Use the first index from the tuple for multi-word tokens\n",
    "            else:\n",
    "                buffer.append(token['id'])\n",
    "\n",
    "    \n",
    "\n",
    "        arcs = []  #(dep,head)\n",
    "\n",
    "        while buffer:\n",
    "            action = self.oracle(stack, buffer, sentence)\n",
    "\n",
    "            features = self.extract_features(stack, buffer, sentence)\n",
    "            transitions.append((features, action))\n",
    "        \n",
    "        \n",
    "            if action == 'SHIFT':\n",
    "                stack.append(buffer.popleft())\n",
    "            elif action == 'LEFT-ARC':\n",
    "                arcs.append((stack[-1], buffer[0]))\n",
    "                stack.pop()\n",
    "            elif action == 'RIGHT-ARC' :\n",
    "                arcs.append((buffer[0], stack[-1]))\n",
    "                buffer.popleft()\n",
    "\n",
    "        return transitions\n",
    "\n",
    "    def extract_features(self, stack, buffer, sentence):\n",
    "    # Initialize default features\n",
    "        features = {\n",
    "        'stack_top_id': 0, 'buffer_first_id': 0,\n",
    "        'stack_top_word': 'NULL', 'buffer_first_word': 'NULL',\n",
    "        'stack_top_pos': 'NULL', 'buffer_first_pos': 'NULL'\n",
    "        }\n",
    "\n",
    "    # Check and assign the top of the stack features\n",
    "        if stack:\n",
    "            stack_top_idx = stack[-1]\n",
    "            stack_top_token = sentence[stack_top_idx - 1]  # Adjust for zero indexing\n",
    "            features['stack_top_id'] = stack_top_idx\n",
    "            features['stack_top_word'] = stack_top_token['form'].lower()\n",
    "            features['stack_top_pos'] = stack_top_token['upos']\n",
    "\n",
    "    # Check and assign the first item in the buffer features\n",
    "        if buffer:\n",
    "            buffer_first_idx = buffer[0]\n",
    "            buffer_first_token = sentence[buffer_first_idx - 1]  # Adjust for zero indexing\n",
    "            features['buffer_first_id'] = buffer_first_idx\n",
    "            features['buffer_first_word'] = buffer_first_token['form'].lower()\n",
    "            features['buffer_first_pos'] = buffer_first_token['upos']\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    # Retrieve the sentence data (list of tuples)\n",
    "        sentence_data = self.data[idx]\n",
    "    \n",
    "    # You might want to process each token in the sentence. \n",
    "    # Here's an example of how you could handle this:\n",
    "        processed_data = []\n",
    "        for token in sentence_data:\n",
    "            if len(token) == 2:\n",
    "                features, action = token\n",
    "                processed_data.append((features, action))\n",
    "            else:\n",
    "                raise ValueError(f\"Expected each token to be a tuple of 2 elements, got {len(token)} elements.\")\n",
    "    \n",
    "    # Return the processed list of tokens\n",
    "        return processed_data\n",
    "\n",
    "# Example usage\n",
    "train_dataset = DependencyParsingDataset('./UD_English-EWT-master/en_ewt-ud-train.conllu')\n",
    "dev_dataset = DependencyParsingDataset('./UD_English-EWT-master/en_ewt-ud-dev.conllu')\n",
    "test_dataset = DependencyParsingDataset('./UD_English-EWT-master/en_ewt-ud-test.conllu')\n",
    "\n",
    "# train_dataset_Hindi = DependencyParsingDataset('./UD_Hindi-HDTB-master/hi_hdtb-ud-train.conllu')\n",
    "# dev_dataset_Hindi = DependencyParsingDataset('./UD_Hindi-HDTB-master/hi_hdtb-ud-dev.conllu')\n",
    "\n",
    "\n",
    "\n",
    "# Example to fetch and print a batch\n",
    "# for features, actions in train_loader:\n",
    "#     print(features, actions)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DependencyParserModel(nn.Module):\n",
    "    def __init__(self, pos_vocab_size, pos_embedding_dim, embedding_dim, hidden_dim, num_actions):\n",
    "        super(DependencyParserModel, self).__init__()\n",
    "        self.pos_embedding = nn.Embedding(pos_vocab_size, pos_embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim * 2 + pos_embedding_dim * 2, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_actions)\n",
    "\n",
    "    def forward(self, stack_top_embeddings, buffer_first_embeddings, stack_top_pos_indices, buffer_first_pos_indices):\n",
    "        # Embed POS tags and expand dimensions to match embeddings\n",
    "        stack_top_pos_embeddings = self.pos_embedding(stack_top_pos_indices).squeeze(1)\n",
    "        buffer_first_pos_embeddings = self.pos_embedding(buffer_first_pos_indices).squeeze(1)\n",
    "\n",
    "        # Combine all embeddings\n",
    "        combined_embeddings = torch.cat((stack_top_embeddings, buffer_first_embeddings, stack_top_pos_embeddings, buffer_first_pos_embeddings), dim=1)\n",
    "        \n",
    "        # Process with LSTM\n",
    "        lstm_out, _ = self.lstm(combined_embeddings.unsqueeze(1))  # Unsqueeze to add a seq_length dimension\n",
    "        logits = self.fc(lstm_out[:, -1, :])\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pos_vocab(dataset):\n",
    "    pos_tags = set()\n",
    "    for sentence_data in dataset:  # Iterate over each sentence data in the dataset\n",
    "        for token in sentence_data:  # Each token is a tuple (features, action)\n",
    "            features, action = token\n",
    "            # Assuming 'features' is a dictionary containing 'stack_top_pos' and 'buffer_first_pos'\n",
    "            pos_tags.add(features['stack_top_pos'])\n",
    "            pos_tags.add(features['buffer_first_pos'])\n",
    "\n",
    "    # Map each POS tag to a unique index\n",
    "    pos_to_index = {pos: idx for idx, pos in enumerate(pos_tags)}\n",
    "    pos_to_index['<PAD>'] = len(pos_to_index)  # Adding a padding token for POS tags\n",
    "    return pos_to_index\n",
    "\n",
    "# Assuming 'DependencyParsingDataset' can be iterated and yields (features, action)\n",
    "pos_vocab = build_pos_vocab(train_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the model directly from Gensim's API\n",
    "word2VecModel = api.load(\"word2vec-google-news-300\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_word_embedding(word, word2VecModel):\n",
    "    try:\n",
    "        return word2VecModel[word]\n",
    "    except KeyError:\n",
    "        # Attempt to remove apostrophes and retry\n",
    "        word = word.replace(\"'\", \"\")\n",
    "        if word in word2VecModel:\n",
    "            return word2VecModel[word]\n",
    "        # Finally, return a zero vector if no suitable word is found\n",
    "        return np.zeros(word2VecModel.vector_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "action_to_index = {\n",
    "    \"SHIFT\": 0,\n",
    "    \"RIGHT-ARC\": 1,\n",
    "    \"LEFT-ARC\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def collate_fn(batch, word2VecModel, pos_vocab, action_to_index):\n",
    "    # Containers for batch data\n",
    "    stack_top_embeddings, buffer_first_embeddings = [], []\n",
    "    stack_top_pos_indices, buffer_first_pos_indices = [], []\n",
    "    actions_indices = []\n",
    "\n",
    "    for sentence in batch:\n",
    "        for features, action in sentence:\n",
    "            # Convert words to embeddings\n",
    "            stack_top_embedding = torch.tensor(get_word_embedding(features['stack_top_word'], word2VecModel), dtype=torch.float)\n",
    "            buffer_first_embedding = torch.tensor(get_word_embedding(features['buffer_first_word'], word2VecModel), dtype=torch.float)\n",
    "\n",
    "            # Convert POS tags to indices\n",
    "            stack_top_pos_index = pos_vocab[features['stack_top_pos']]\n",
    "            buffer_first_pos_index = pos_vocab[features['buffer_first_pos']]\n",
    "\n",
    "            # Append embeddings and POS indices separately\n",
    "            stack_top_embeddings.append(stack_top_embedding)\n",
    "            buffer_first_embeddings.append(buffer_first_embedding)\n",
    "            stack_top_pos_indices.append(torch.tensor([stack_top_pos_index], dtype=torch.long))\n",
    "            buffer_first_pos_indices.append(torch.tensor([buffer_first_pos_index], dtype=torch.long))\n",
    "\n",
    "            # Convert action to index and append\n",
    "            actions_indices.append(action_to_index[action])\n",
    "\n",
    "    # Pad sequences\n",
    "    stack_top_embeddings = pad_sequence(stack_top_embeddings, batch_first=True, padding_value=0.0)\n",
    "    buffer_first_embeddings = pad_sequence(buffer_first_embeddings, batch_first=True, padding_value=0.0)\n",
    "    stack_top_pos_indices = pad_sequence(stack_top_pos_indices, batch_first=True, padding_value=pos_vocab['<PAD>'])\n",
    "    buffer_first_pos_indices = pad_sequence(buffer_first_pos_indices, batch_first=True, padding_value=pos_vocab['<PAD>'])\n",
    "\n",
    "    actions_indices = torch.tensor(actions_indices, dtype=torch.long)\n",
    "\n",
    "    return (stack_top_embeddings, buffer_first_embeddings, stack_top_pos_indices, buffer_first_pos_indices), actions_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack Top Embeddings Shape: torch.Size([554, 300])\n",
      "Buffer First Embeddings Shape: torch.Size([554, 300])\n",
      "Stack Top POS Indices Shape: torch.Size([554, 1])\n",
      "Buffer First POS Indices Shape: torch.Size([554, 1])\n",
      "Actions Indices Shape: torch.Size([554])\n",
      "Actions Indices: tensor([0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 2, 0, 0, 0, 1, 1, 1, 0, 0, 2, 2, 0, 0, 2, 1, 0, 1, 0, 2, 1, 0, 2,\n",
      "        0, 0, 2, 1, 1, 0, 0, 2, 0, 1, 1, 0, 0, 1, 2, 0, 0, 0, 2, 2, 1, 0, 2, 1,\n",
      "        0, 0, 2, 2, 1, 0, 0, 2, 0, 0, 0, 0, 2, 2, 2, 0, 1, 0, 2, 1, 1, 0, 0, 0,\n",
      "        1, 1, 2, 2, 0, 0, 0, 2, 2, 1, 1, 0, 2, 0, 0, 2, 1, 0, 0, 2, 2, 2, 0, 0,\n",
      "        2, 1, 1, 0, 0, 2, 1, 1, 0, 0, 0, 2, 2, 2, 0, 0, 0, 1, 0, 2, 2, 2, 1, 0,\n",
      "        0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 2, 2, 1, 1,\n",
      "        1, 0, 1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 1, 0, 2,\n",
      "        0, 0, 2, 1, 0, 0, 2, 2, 1, 0, 2, 1, 0, 0, 0, 2, 1, 0, 0, 2, 2, 0, 1, 0,\n",
      "        0, 2, 0, 0, 1, 0, 2, 1, 0, 2, 1, 0, 0, 2, 2, 0, 1, 0, 2, 0, 0, 2, 1, 0,\n",
      "        0, 0, 2, 0, 1, 0, 2, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 2,\n",
      "        2, 0, 0, 2, 1, 0, 2, 1, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 2, 1, 1, 0, 2, 0,\n",
      "        0, 2, 1, 0, 0, 2, 2, 0, 0, 2, 1, 1, 0, 0, 1, 2, 2, 0, 0, 0, 2, 2, 1, 0,\n",
      "        0, 0, 0, 2, 2, 2, 0, 1, 0, 0, 0, 2, 2, 2, 1, 0, 0, 0, 0, 2, 2, 0, 0, 2,\n",
      "        1, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 2, 2, 2, 0, 0, 0, 2, 0, 1, 0, 2, 0, 0,\n",
      "        0, 0, 2, 1, 2, 2, 1, 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 2, 2, 2, 1,\n",
      "        0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 1, 1, 0, 2, 0, 1, 0, 0, 2, 2,\n",
      "        1, 0, 2, 0, 0, 0, 2, 2, 0, 1, 0, 0, 0, 2, 2, 0, 1, 1, 0, 2, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,\n",
      "        0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 2, 2, 2, 1, 1, 0, 0, 2, 2, 0, 1, 0, 0, 2,\n",
      "        2, 0, 0, 2, 1, 0, 0, 0, 2, 2, 2, 2, 2, 0, 1, 0, 2, 0, 1, 0, 2, 1, 0, 0,\n",
      "        2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 2, 2, 2, 1, 0, 0, 2, 2, 1, 0, 0, 0, 0, 2,\n",
      "        2, 2, 2, 0, 0, 0, 2, 2, 1, 0, 0, 0, 2, 2, 0, 0, 0, 2, 2, 1, 0, 0, 2, 2,\n",
      "        1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Example DataLoader usage\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
    "                          collate_fn=lambda batch: collate_fn(batch, word2VecModel, pos_vocab, action_to_index))\n",
    "\n",
    "val_loader = DataLoader(dev_dataset, batch_size=32, shuffle=True,\n",
    "                          collate_fn=lambda batch: collate_fn(batch, word2VecModel, pos_vocab, action_to_index))\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True, \n",
    "                         collate_fn=lambda batch: collate_fn(batch, word2VecModel, pos_vocab, action_to_index))\n",
    "for batch in train_loader:\n",
    "    # Unpack the batch data\n",
    "    (stack_top_embeddings, buffer_first_embeddings, stack_top_pos_indices, buffer_first_pos_indices), actions_indices = batch\n",
    "    \n",
    "    # Print shapes and type of data in the batch\n",
    "    print(\"Stack Top Embeddings Shape:\", stack_top_embeddings.shape)\n",
    "    print(\"Buffer First Embeddings Shape:\", buffer_first_embeddings.shape)\n",
    "    print(\"Stack Top POS Indices Shape:\", stack_top_pos_indices.shape)\n",
    "    print(\"Buffer First POS Indices Shape:\", buffer_first_pos_indices.shape)\n",
    "    print(\"Actions Indices Shape:\", actions_indices.shape)\n",
    "    print(\"Actions Indices:\", actions_indices)\n",
    "\n",
    "    # Optionally, break after the first batch to just see one example\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DependencyParserModel(\n",
       "  (pos_embedding): Embedding(19, 50)\n",
       "  (lstm): LSTM(700, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming you've predefined the following variables\n",
    "pos_vocab_size = len(pos_vocab)  # from your POS vocabulary\n",
    "pos_embedding_dim = 50  # arbitrary choice, can be tuned\n",
    "embedding_dim = 300  # assuming your word embeddings are of size 300\n",
    "hidden_dim = 128  # hidden dimension of the LSTM\n",
    "num_actions = 3  # \"SHIFT\", \"RIGHT-ARC\", \"LEFT-ARC\"\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize model\n",
    "model = DependencyParserModel(pos_vocab_size, pos_embedding_dim, embedding_dim, hidden_dim, num_actions)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5706\n",
      "Epoch 2/10, Loss: 0.5057\n",
      "Epoch 3/10, Loss: 0.4894\n",
      "Epoch 4/10, Loss: 0.4782\n",
      "Epoch 5/10, Loss: 0.4679\n",
      "Epoch 6/10, Loss: 0.4600\n",
      "Epoch 7/10, Loss: 0.4526\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loss and optimizer\n",
    "num_epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Example training loop for one epoch\n",
    "model.train()  # Set the model to training mode\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for (stack_top_embeddings, buffer_first_embeddings, stack_top_pos_indices, buffer_first_pos_indices), actions_indices in train_loader:\n",
    "        # Move tensors to the appropriate device\n",
    "        stack_top_embeddings = stack_top_embeddings.to(device)\n",
    "        buffer_first_embeddings = buffer_first_embeddings.to(device)\n",
    "        stack_top_pos_indices = stack_top_pos_indices.to(device)\n",
    "        buffer_first_pos_indices = buffer_first_pos_indices.to(device)\n",
    "        actions_indices = actions_indices.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform a forward pass through the model\n",
    "        outputs = model(stack_top_embeddings, buffer_first_embeddings, stack_top_pos_indices, buffer_first_pos_indices)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, actions_indices)\n",
    "\n",
    "        # Backpropagate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(predicted, true):\n",
    "    precision = precision_score(true, predicted, average='weighted')\n",
    "    recall = recall_score(true, predicted, average='weighted')\n",
    "    f1 = f1_score(true, predicted, average='weighted')\n",
    "    return precision, recall, f1\n",
    "\n",
    "def validate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predicted = []\n",
    "    all_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (stack_top_embeddings, buffer_first_embeddings, stack_top_pos_indices, buffer_first_pos_indices), actions_indices in data_loader:\n",
    "            stack_top_embeddings = stack_top_embeddings.to(device)\n",
    "            buffer_first_embeddings = buffer_first_embeddings.to(device)\n",
    "            stack_top_pos_indices = stack_top_pos_indices.to(device)\n",
    "            buffer_first_pos_indices = buffer_first_pos_indices.to(device)\n",
    "            actions_indices = actions_indices.to(device)\n",
    "\n",
    "            outputs = model(stack_top_embeddings, buffer_first_embeddings, stack_top_pos_indices, buffer_first_pos_indices)\n",
    "\n",
    "            loss = criterion(outputs, actions_indices)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predicted.extend(predicted.cpu().numpy())\n",
    "            all_true.extend(actions_indices.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    precision, recall, f1 = calculate_metrics(all_predicted, all_true)\n",
    "    return avg_loss, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model on the validation data\n",
    "val_loss, val_precision, val_recall, val_f1 = validate_model(model, val_loader, criterion, device)\n",
    "print(f\"Validation Loss: {val_loss:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# Test the model on the test data\n",
    "test_loss, test_precision, test_recall, test_f1 = validate_model(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1 Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_Hindi = DependencyParsingDataset('./UD_Hindi-HDTB-master/hi_hdtb-ud-train.conllu')\n",
    "dev_dataset_Hindi = DependencyParsingDataset('./UD_Hindi-HDTB-master/hi_hdtb-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Download the FastText Hindi word vectors from the FastText website\n",
    "# Ensure the file 'cc.hi.300.vec.gz' is in your current working directory\n",
    "\n",
    "# Load the FastText Hindi word vectors\n",
    "hindi_model_path = \"C:/Users/Rithin/Downloads/cc.hi.300.vec/cc.hi.300.vec\" # Path to the downloaded file\n",
    "hindi_word2vec_model = KeyedVectors.load_word2vec_format(hindi_model_path, binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vocab = build_pos_vocab(train_dataset_Hindi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DependencyParserModel(\n",
       "  (pos_embedding): Embedding(19, 50)\n",
       "  (lstm): LSTM(700, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_hindi = DependencyParserModel(pos_vocab_size, pos_embedding_dim, embedding_dim, hidden_dim, num_actions)\n",
    "model_hindi.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack Top Embeddings Shape: torch.Size([1042, 300])\n",
      "Buffer First Embeddings Shape: torch.Size([1042, 300])\n",
      "Stack Top POS Indices Shape: torch.Size([1042, 1])\n",
      "Buffer First POS Indices Shape: torch.Size([1042, 1])\n",
      "Actions Indices Shape: torch.Size([1042])\n",
      "Actions Indices: tensor([0, 0, 0,  ..., 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Example DataLoader usage\n",
    "train_loader_hindi = DataLoader(train_dataset_Hindi, batch_size=32, shuffle=True,\n",
    "                          collate_fn=lambda batch: collate_fn(batch, hindi_word2vec_model, pos_vocab, action_to_index))\n",
    "\n",
    "val_loader_hindi = DataLoader(dev_dataset_Hindi, batch_size=32, shuffle=True,\n",
    "                          collate_fn=lambda batch: collate_fn(batch, hindi_word2vec_model, pos_vocab, action_to_index))\n",
    "for batch in train_loader_hindi:\n",
    "    # Unpack the batch data\n",
    "    (stack_top_embeddings, buffer_first_embeddings, stack_top_pos_indices, buffer_first_pos_indices), actions_indices = batch\n",
    "    \n",
    "    # Print shapes and type of data in the batch\n",
    "    print(\"Stack Top Embeddings Shape:\", stack_top_embeddings.shape)\n",
    "    print(\"Buffer First Embeddings Shape:\", buffer_first_embeddings.shape)\n",
    "    print(\"Stack Top POS Indices Shape:\", stack_top_pos_indices.shape)\n",
    "    print(\"Buffer First POS Indices Shape:\", buffer_first_pos_indices.shape)\n",
    "    print(\"Actions Indices Shape:\", actions_indices.shape)\n",
    "    print(\"Actions Indices:\", actions_indices)\n",
    "\n",
    "    # Optionally, break after the first batch to just see one example\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.4161\n",
      "Epoch 2/30, Loss: 0.3534\n",
      "Epoch 3/30, Loss: 0.3355\n",
      "Epoch 4/30, Loss: 0.3245\n",
      "Epoch 5/30, Loss: 0.3157\n",
      "Epoch 6/30, Loss: 0.3093\n",
      "Epoch 7/30, Loss: 0.3032\n",
      "Epoch 8/30, Loss: 0.2983\n",
      "Epoch 9/30, Loss: 0.2935\n",
      "Epoch 10/30, Loss: 0.2882\n",
      "Epoch 11/30, Loss: 0.2844\n",
      "Epoch 12/30, Loss: 0.2806\n",
      "Epoch 13/30, Loss: 0.2768\n",
      "Epoch 14/30, Loss: 0.2727\n",
      "Epoch 15/30, Loss: 0.2688\n",
      "Epoch 16/30, Loss: 0.2652\n",
      "Epoch 17/30, Loss: 0.2625\n",
      "Epoch 18/30, Loss: 0.2589\n",
      "Epoch 19/30, Loss: 0.2561\n",
      "Epoch 20/30, Loss: 0.2526\n",
      "Epoch 21/30, Loss: 0.2501\n",
      "Epoch 22/30, Loss: 0.2464\n",
      "Epoch 23/30, Loss: 0.2441\n",
      "Epoch 24/30, Loss: 0.2422\n",
      "Epoch 25/30, Loss: 0.2392\n",
      "Epoch 26/30, Loss: 0.2364\n",
      "Epoch 27/30, Loss: 0.2339\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(stack_top_embeddings, buffer_first_embeddings, stack_top_pos_indices, buffer_first_pos_indices)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Backpropagate the gradients\u001b[39;00m\n\u001b[0;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Rithin\\Desktop\\S24\\NLP\\Grad-level-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Rithin\\Desktop\\S24\\NLP\\Grad-level-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Rithin\\Desktop\\S24\\NLP\\Grad-level-project\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Rithin\\Desktop\\S24\\NLP\\Grad-level-project\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs =10\n",
    "model_hindi.train()  # Set the model to training mode\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for (stack_top_embeddings, buffer_first_embeddings, stack_top_pos_indices, buffer_first_pos_indices), actions_indices in train_loader_hindi:\n",
    "        # Move tensors to the appropriate device\n",
    "        stack_top_embeddings = stack_top_embeddings.to(device)\n",
    "        buffer_first_embeddings = buffer_first_embeddings.to(device)\n",
    "        stack_top_pos_indices = stack_top_pos_indices.to(device)\n",
    "        buffer_first_pos_indices = buffer_first_pos_indices.to(device)\n",
    "        actions_indices = actions_indices.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform a forward pass through the model\n",
    "        outputs = model(stack_top_embeddings, buffer_first_embeddings, stack_top_pos_indices, buffer_first_pos_indices)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, actions_indices)\n",
    "\n",
    "        # Backpropagate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_loader_hindi)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "val_loss, val_accuracy = validate_model(model_hindi, val_loader_hindi, criterion, device)\n",
    "print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
